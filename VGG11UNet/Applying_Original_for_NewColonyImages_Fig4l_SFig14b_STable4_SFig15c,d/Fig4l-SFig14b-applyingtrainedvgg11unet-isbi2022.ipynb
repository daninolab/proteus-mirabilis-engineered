{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Application of Trained VGG-11 U-Net for Predicting Ring Boundaries on Newly Acquired Images\n\nThis script applies our previously optimized VGG-11 U-Net segmentation model, as presented in Doshi & Shaw et al. 2022, to generate ring boundary masks for our newly acquired *Proteus mirabilis* colony images that were not used in the original training, validation, and testing of the model<sup>1,2</sup>. Namely, we have used this model to segment rings on images of our copper-sensing strain pCopA-*flgM*, as well as one of our iptg-sensing strains, pLac-*flgM*, grown at different temperatures. As with that of our original implementation, this script utilizes various elements from \"Segmentation Models: Python library with Neural Networks for Image Segmentation based on PyTorch\" (SMP), including utility functions defined in the SMP car segmentation example<sup>3</sup>. \n\n[1] Doshi, A.\\*\\, M. Shaw\\*\\, R. Tonea, R. Minyety, S. Moon, A. Laine, J. Guo\\^\\, and T. Danino\\^\\. A deep learning pipeline for segmentation of *Proteus mirabilis* colony patterns. in *2022 IEEE 19th\nInternational Symposium on Biomedical Imaging (ISBI)*. 2022. IEEE. doi: 10.1109/ISBI52829.2022.9761643\n\n[2] daninolab. mirabilis-ringboundary-seg-minimal. 2022; Available from: https://github.com/daninolab/proteus-mirabilis.\n\n[3] Iakubovskii, P. segmentation_models.pytorch (Version 0.2.0). 2021; Available from: https://github.com/qubvel/segmentation_models.pytorch.","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# Earlier PyPI version (0.2.0) that we have been using: \n!pip install segmentation-models-pytorch==0.2.0\n# To get the latest version from source:\n#!pip install git+https://github.com/qubvel/segmentation_models.pytorch","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:50:28.082449Z","iopub.execute_input":"2023-02-01T23:50:28.083049Z","iopub.status.idle":"2023-02-01T23:50:39.574448Z","shell.execute_reply.started":"2023-02-01T23:50:28.082932Z","shell.execute_reply":"2023-02-01T23:50:39.573504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport csv\nimport copy\nimport time\nfrom tqdm import tqdm\nimport os\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision import models\nfrom torch.utils.data import Dataset, DataLoader\nimport glob\nimport matplotlib.pyplot as plt\nfrom matplotlib import pylab as pl\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch import losses\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\nimport albumentations as albu\nfrom skimage.morphology import skeletonize, thin\nfrom skimage import data\nfrom skimage.util import invert","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:06.463365Z","iopub.execute_input":"2023-02-01T23:51:06.463645Z","iopub.status.idle":"2023-02-01T23:51:11.682939Z","shell.execute_reply.started":"2023-02-01T23:51:06.463621Z","shell.execute_reply":"2023-02-01T23:51:11.681776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:14.385410Z","iopub.execute_input":"2023-02-01T23:51:14.385854Z","iopub.status.idle":"2023-02-01T23:51:14.391183Z","shell.execute_reply.started":"2023-02-01T23:51:14.385824Z","shell.execute_reply":"2023-02-01T23:51:14.390157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# Set path to folder of new images to generate predicitions for\nimg_dir = '../input/101322metals' \n        # '../input/101522metals' \n        # '../input/101622metals' \n        # '../input/34C_gfp_flgm_chew_0i_10i'\n        # '../input/36C_gfp_flgm_chew_0i_10i'\n        # '../input/37C_gfp_flgm_chew_0i_10i'","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:17.384015Z","iopub.execute_input":"2023-02-01T23:51:17.384322Z","iopub.status.idle":"2023-02-01T23:51:17.388162Z","shell.execute_reply.started":"2023-02-01T23:51:17.384295Z","shell.execute_reply":"2023-02-01T23:51:17.387295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get list of the files names (to print when displaying images later)\nimg_list = [idx for idx in os.listdir(img_dir) if idx.endswith('.tif')]\nimg_list.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:18.353329Z","iopub.execute_input":"2023-02-01T23:51:18.353931Z","iopub.status.idle":"2023-02-01T23:51:18.388345Z","shell.execute_reply.started":"2023-02-01T23:51:18.353795Z","shell.execute_reply":"2023-02-01T23:51:18.387254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print how many images we're working with\nnum_imgs = len(img_list)\nprint(num_imgs)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:19.285312Z","iopub.execute_input":"2023-02-01T23:51:19.285727Z","iopub.status.idle":"2023-02-01T23:51:19.290420Z","shell.execute_reply.started":"2023-02-01T23:51:19.285700Z","shell.execute_reply":"2023-02-01T23:51:19.289516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset class\n# note: this doesn't include ground truth masks anymore\nclass BacteriaDataset(Dataset):\n    \n    CLASSES = ['boundaries']\n    \n    def __init__(self, img_IDs, img_dir, classes=None, augmentation=None, preprocessing=None):\n        self.img_IDs = img_IDs\n        self.img_dir = img_dir\n        self.augmentation = augmentation         # for augmentations\n        self.preprocessing = preprocessing       # preprocessing to normalize images\n        self.imgs_fps = [os.path.join(self.img_dir, img_id) for img_id in self.img_IDs]\n        \n         # convert str names to class values on masks\n        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n        \n    def __len__(self):\n        return len(self.img_IDs)\n\n    def __getitem__(self, i):\n        \n        # read data\n        img = cv2.imread(self.imgs_fps[i])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=img)\n            img = sample['image']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=img)\n            img = sample['image']\n            \n        return img","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:20.247397Z","iopub.execute_input":"2023-02-01T23:51:20.247717Z","iopub.status.idle":"2023-02-01T23:51:20.256350Z","shell.execute_reply.started":"2023-02-01T23:51:20.247692Z","shell.execute_reply":"2023-02-01T23:51:20.255239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define transformations\n# For training set (not used here):\ndef get_training_augmentation():\n    train_transform = [albu.PadIfNeeded(min_height=1024, min_width=1024, always_apply=True, border_mode=cv2.BORDER_REFLECT_101),\n                       albu.Rotate(limit=(-10,10), border_mode=cv2.BORDER_REFLECT_101, p=0.5),\n                       albu.HorizontalFlip(p=0.5),\n                       albu.VerticalFlip(p=0.5),\n                       albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0, rotate_limit=0,\n                                          border_mode=cv2.BORDER_REFLECT_101, p=0.5), # translate\n                       albu.ShiftScaleRotate(shift_limit=0, scale_limit=0.5, rotate_limit=0,\n                                          border_mode=cv2.BORDER_REFLECT_101, p=0.5), # zoom\n                      ]\n    return albu.Compose(train_transform)\n\n# For validation and test sets \n# (necessary for resizing images to feed into model):\ndef get_val_test_augmentation():\n    val_test_transform = [\n                       albu.PadIfNeeded(min_height=1024, min_width=1024, always_apply=True, border_mode=cv2.BORDER_REFLECT_101),\n                      ]\n    return albu.Compose(val_test_transform)\n\n# Necessary for feeding images into model\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing(preprocessing_fn):\n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor),\n    ]\n    return albu.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:51.335495Z","iopub.execute_input":"2023-02-01T23:51:51.335952Z","iopub.status.idle":"2023-02-01T23:51:51.342940Z","shell.execute_reply.started":"2023-02-01T23:51:51.335921Z","shell.execute_reply":"2023-02-01T23:51:51.342383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(15, 10))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image,cmap='binary',vmin=0,vmax=1)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:23.739541Z","iopub.execute_input":"2023-02-01T23:51:23.739861Z","iopub.status.idle":"2023-02-01T23:51:23.746552Z","shell.execute_reply.started":"2023-02-01T23:51:23.739831Z","shell.execute_reply":"2023-02-01T23:51:23.745293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for generating predicted mask (cropped back down to the size of originak image: 1000x1000)\n# & skeletonized version of cropped predicted mask\n# ...given an index, a dataset, & a model\n\ndef generate_prediction_skel(n, dataset, model):\n    # Get transformed (padded) + preprocessed image\n    image = dataset[n] \n    img_tensor = torch.from_numpy(image).to(device).unsqueeze(0)\n    \n    # Generate prediction\n    pr_mask = model.predict(img_tensor)\n    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n    cropped_pr_mask = pr_mask[12:1012, 12:1012]\n    \n    # Skeletonize the mask\n    skeleton = skeletonize(cropped_pr_mask)\n    skeleton = skeleton.astype(np.float32)\n    \n    return cropped_pr_mask, skeleton","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:27.202639Z","iopub.execute_input":"2023-02-01T23:51:27.203162Z","iopub.status.idle":"2023-02-01T23:51:27.207761Z","shell.execute_reply.started":"2023-02-01T23:51:27.203129Z","shell.execute_reply":"2023-02-01T23:51:27.207203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load in Model","metadata":{}},{"cell_type":"code","source":"# Variables for initializing the previously trained model \nEncoder = 'vgg11'\nWeights = 'imagenet'\nACTIVATION = 'sigmoid'\nAttention = None \nCLASSES = ['boundaries']\npreprocess_input = get_preprocessing_fn(Encoder, Weights)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:29.737064Z","iopub.execute_input":"2023-02-01T23:51:29.737399Z","iopub.status.idle":"2023-02-01T23:51:29.741696Z","shell.execute_reply.started":"2023-02-01T23:51:29.737367Z","shell.execute_reply":"2023-02-01T23:51:29.740780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for loading in the previously trained model \ndef load_model(checkpoint_path):\n    # Initialize the model (optimizer not needed for inference)\n    model = smp.Unet(\n        encoder_name=Encoder, \n        encoder_weights=Weights, \n        decoder_attention_type=Attention,\n        in_channels=3, \n        classes=len(CLASSES), \n        activation=ACTIVATION,\n    )\n    \n    # Load in the checkpoint\n    checkpoint = torch.load(checkpoint_path)\n    \n    # Load in the model's learned parameters\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:30.523707Z","iopub.execute_input":"2023-02-01T23:51:30.524184Z","iopub.status.idle":"2023-02-01T23:51:30.528846Z","shell.execute_reply.started":"2023-02-01T23:51:30.524151Z","shell.execute_reply":"2023-02-01T23:51:30.528193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the path to the previously trained model & load it in\ncheckpoint_path = '../input/best-unets-earlystopping/final_models_to_test/vgg11_UNet_cp_noaug_300refined_082021_epoch_34.pth'\nmodel = load_model(checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:34.605912Z","iopub.execute_input":"2023-02-01T23:51:34.606366Z","iopub.status.idle":"2023-02-01T23:51:42.113964Z","shell.execute_reply.started":"2023-02-01T23:51:34.606340Z","shell.execute_reply":"2023-02-01T23:51:42.112909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate predicted masks","metadata":{}},{"cell_type":"code","source":"# Dataset without transformations/preprocessing for image visualization\ndataset_vis = BacteriaDataset(img_list, img_dir, classes=['boundaries'],)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:43.637397Z","iopub.execute_input":"2023-02-01T23:51:43.639073Z","iopub.status.idle":"2023-02-01T23:51:43.644004Z","shell.execute_reply.started":"2023-02-01T23:51:43.639040Z","shell.execute_reply":"2023-02-01T23:51:43.642882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessed dataset (no augmentations) for feeding into model\ndataset = BacteriaDataset(img_list, img_dir, classes=['boundaries'],\n                          augmentation=get_val_test_augmentation(),\n                          preprocessing=get_preprocessing(preprocess_input),)\n\n# Preprocessed dataset loader\ntest_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:57.372937Z","iopub.execute_input":"2023-02-01T23:51:57.373523Z","iopub.status.idle":"2023-02-01T23:51:57.377764Z","shell.execute_reply.started":"2023-02-01T23:51:57.373483Z","shell.execute_reply":"2023-02-01T23:51:57.377101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore DataLoader\nprint('\\nData Info:')\ndataiter = iter(test_loader)\ndata = dataiter.next()\nimages = data\nprint(\"shape of images : {}\".format(images.shape))","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:51:59.842464Z","iopub.execute_input":"2023-02-01T23:51:59.842968Z","iopub.status.idle":"2023-02-01T23:52:00.044267Z","shell.execute_reply.started":"2023-02-01T23:51:59.842935Z","shell.execute_reply":"2023-02-01T23:52:00.043392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create output folder for storing cropped predicted masks\npred_folder = 'predictions'\nif not os.path.exists(pred_folder):\n    os.makedirs(pred_folder)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:52:01.267964Z","iopub.execute_input":"2023-02-01T23:52:01.268271Z","iopub.status.idle":"2023-02-01T23:52:01.272486Z","shell.execute_reply.started":"2023-02-01T23:52:01.268244Z","shell.execute_reply":"2023-02-01T23:52:01.271522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create output folder for storing skeletonized cropped predicted masks\nskel_folder = 'skel_predictions'\nif not os.path.exists(skel_folder):\n    os.makedirs(skel_folder)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:52:02.144233Z","iopub.execute_input":"2023-02-01T23:52:02.144769Z","iopub.status.idle":"2023-02-01T23:52:02.148308Z","shell.execute_reply.started":"2023-02-01T23:52:02.144710Z","shell.execute_reply":"2023-02-01T23:52:02.147690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate and save cropped predicted masks (& skeletonized versions) \n# (show every 10th image)\nfor n in range(num_imgs):\n    \n    # generate and save predicted mask & skeleton\n    filename = img_list[n]\n    filename_wo_ext = os.path.splitext(os.path.basename(filename))[0]\n    \n    cropped_pr_mask, skeleton = generate_prediction_skel(n, dataset, model)\n    pred_filename = filename_wo_ext + '_pred.tif'\n    pred_path = os.path.join(pred_folder, pred_filename)\n    cv2.imwrite(pred_path, cropped_pr_mask)\n    \n    skel_filename = filename_wo_ext + '_skel.tif'\n    skel_path = os.path.join(skel_folder, skel_filename)\n    cv2.imwrite(skel_path, skeleton)\n    \n    # Show every 10th image\n    if (n % 10 == 0):\n        \n        # So we know which image we're viewing\n        print(filename) \n        image_vis = dataset_vis[n]\n\n        # Visualize original image, cropped predicted mask, & skeletonized version\n        visualize(original_pattern_image=image_vis/255,\n                predicted_mask=cropped_pr_mask,\n                skeletonized_predicted_mask=skeleton,)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:52:19.367910Z","iopub.execute_input":"2023-02-01T23:52:19.368433Z","iopub.status.idle":"2023-02-01T23:52:28.988075Z","shell.execute_reply.started":"2023-02-01T23:52:19.368392Z","shell.execute_reply":"2023-02-01T23:52:28.986868Z"},"trusted":true},"execution_count":null,"outputs":[]}]}