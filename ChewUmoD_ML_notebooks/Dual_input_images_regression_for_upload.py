# -*- coding: utf-8 -*-
"""01_25_23_Dual Input Images Regression For Upload

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E8xEIVUIZnADIoV6bCxWW4iPMu5-IkVo
"""

# NOTE: IN this notebook, you have to start, then restart runtime. Otherwise tensorflow isn't the right version

import os
import shutil
import time
import math
import random
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm
from pprint import pprint
import pathlib
import PIL
from PIL import Image, ImageOps
import matplotlib.pyplot as plt
import pickle

import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator

from IPython.display import clear_output
import cv2 as cv
from sklearn import model_selection

# %config Completer.use_jedi = False
# torch.manual_seed(0)

# Get the date for later saving
from datetime import date
today = date.today()
date_string = today.strftime("%m%d%y")

!pip  install tensorflow-gpu
!pip install tensorflow==2.8
!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2
from tensorflow.keras.applications import *
from tensorflow.keras import models, layers, callbacks

#Use this to check if the GPU is configured correctly
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

print(tf.version.VERSION)

IMG_SIZE = 512

def convert_all_to_jpgs(source_folder, target_folder=None, delete_existing_jpg_folder=True, source_ext=".tiff"):

    target_folder = source_folder + "_jpg"
    if delete_existing_jpg_folder and os.path.exists(target_folder):
        shutil.rmtree(target_folder)
    os.mkdir(target_folder)

    for root, dirs, files in os.walk(source_folder, topdown=False):
        for dirname in dirs:
            os.mkdir(f'{target_folder}/{dirname}')
            
    for root, dirs, files in os.walk(source_folder, topdown=False):
        for name in files:
            infile = os.path.join(root, name)
            if '.tiff' in infile:
                outfile = infile.replace(source_folder, target_folder).replace('.tiff', ".jpg")
                with open(outfile, 'w+') as f:
                    im = Image.open(infile)
                    im.thumbnail(im.size)
                    im.save(f, "JPEG", quality=100)

# Mount my google drive
from google.colab import drive # will need to use verification code here
drive.mount('/content/gdrive', force_remount = True)

# Import all the images into the local environment i guess?
# May need to convert them to grayscale & size 512

def get_available_image_paths(data_root):
  #Get all image paths from my Drive
  all_image_paths = [str(path) for path in data_root.glob('**/*.jpg')]
  print('We have {} images'.format(len(all_image_paths)))
  print(all_image_paths[:2]) #get a sense of where the images are
  return all_image_paths



def load_and_preprocess_image(path, img_size = IMG_SIZE):
  #Can't use tensorflow preprocessing because tifs
  img = plt.imread(path)
  
  img = tf.image.resize(img, [img_size, img_size])
  
  # img /= 255.0  # normalize pixels to 0,1
  return img

root_path = 'gdrive/My Drive/Danino_Lab/Patterning_Scans/cheW_umoD_Expts/Updated_ML_Dataset_103022/All_Ims_For_Upload'
data_root = pathlib.Path(root_path)
target_folder = 'images_transformed_512'
if not os.path.exists(target_folder):
  os.mkdir(target_folder)

# Get list of folders in the root folder & replicate in the local folder
for root, dirs, files in os.walk(data_root, topdown=False):
        for dirname in dirs:
          if not os.path.exists(f'{target_folder}/{dirname}'):
              os.mkdir(f'{target_folder}/{dirname}')
              print(dirname)

image_paths = get_available_image_paths(data_root)
for i, image_path in enumerate(image_paths):
  try:
    im = Image.open(image_path)
    im.thumbnail((512, 512))
    # gray_image = ImageOps.grayscale(im)
    # processed = load_and_preprocess_image(image_path, img_size = 512)
    newpath = image_path.replace('gdrive/My Drive/Danino_Lab/Patterning_Scans/cheW_umoD_Expts/Updated_ML_Dataset_103022/All_Ims_For_Upload', 'images_transformed_512/')
    # gray_image.save(newpath)
    im.save(newpath)
  except Exception as e:
    print(e)
    print(image_path)
    break

# now let's get the labels
new_image_paths = get_available_image_paths(pathlib.Path('images_transformed_512'))
print(new_image_paths[0:3])

# Let's set from the get go whether we want to normalize the labels or not
normalize_conc_labels = True

# # let's make a dict for the labels


if normalize_conc_labels:
  class_dict = {"0_iptg_0.1_ara": [0, 0.5],
              "0_iptg_0.2_ara": [0, 1],
              "0_iptg_0_ara": [0, 0],
              "2.5_iptg_0.1_ara": [0.5, 0.5],
              "2.5_iptg_0.2_ara": [0.5, 1],
              "2.5_iptg_0_ara": [0.5, 0],
              "5_iptg_0.1_ara": [1.0, 0.5],
              "5_iptg_0.2_ara": [1.0, 1],
              "5_iptg_0_ara": [1.0, 0]}

else:
  class_dict = {"0_iptg_0.1_ara": [0, 0.1],
                "0_iptg_0.2_ara": [0, 0.2],
                "0_iptg_0_ara": [0, 0],
                "2.5_iptg_0.1_ara": [2.5, 0.1],
                "2.5_iptg_0.2_ara": [2.5, 0.2],
                "2.5_iptg_0_ara": [2.5, 0],
                "5_iptg_0.1_ara": [5.0, 0.1],
                "5_iptg_0.2_ara": [5.0, 0.2],
                "5_iptg_0_ara": [5.0, 0]}

for key in class_dict:
  print(key)
  print(class_dict[key])

print(os.listdir('images_transformed_512'))
total = 0
for root, dirs, main_files in os.walk('images_transformed_512'):
    for dir in dirs:
      print(dir)
      if not  "Intermediate" in dir:
        print(class_dict[dir])
        for img_root, img_dirs, img_files in os.walk(os.path.join('images_transformed_512', dir)):
          for img_file in img_files:
            total += 1

print("Num ims for training")
print(total)

# Get number of images per class
total = 0
for root, dirs, main_files in os.walk('images_transformed_512'):
    for dir in dirs:
      print(dir)
      if not  "Intermediate" in dir:
        # print(class_dict[dir])
        for img_root, img_dirs, img_files in os.walk(os.path.join('images_transformed_512', dir)):
          for img_file in img_files:
            total += 1
      print(total)
      total = 0

# get the new image paths

data_dir = pathlib.Path('/content/images_transformed_512')
new_image_paths = [str(path) for path in data_dir.glob('*/*.jpg')]
print(new_image_paths[0:3])


PIL.Image.open(str(list(data_dir.glob('*/*.jpg'))[0]))

"""# Importing & labeling the data

"""

# Make the main training set & intermediate evaluation set at same time

X_full = []
y_full = []
img_paths_full = []

X_full_int = []
y_full_int = []
img_paths_full_int = []

for img_path in new_image_paths:
  if not '091419' in img_path:
    if not  "Intermediate" in img_path:
      img_dir = img_path.split('/')[3]
      img_number = class_dict[img_dir]
      # preprocess image for inceptionv3
      im = cv.imread(img_path)
      im = cv.resize(im, (IMG_SIZE, IMG_SIZE))
      # img_processed = tf.keras.applications.inception_v3.preprocess_input(im, data_format=None)
      # img_processed = load_and_preprocess_image(image_path, img_size = 512)
      
      # make dataset in full
      X_full.append(im)

      # for regression
      y_full.append(img_number)

      # in case we need it later, save the image path
      img_paths_full.append(img_path)

    else:
      # This is a diff concentration image
      # Get the IPTG, ara
      img_name = img_path.split('/')[4]
      img_name_split = img_name.split('_')
      img_number = [float(img_name_split[0]), float(img_name_split[2])]
      # decide whether we need to normalize or not:
      if normalize_conc_labels:
        img_number[0] = img_number[0]/5.0
        img_number[1] = img_number[1]/0.2

      # resize image and add to matrix
      im = cv.imread(img_path)
      im = cv.resize(im, (IMG_SIZE, IMG_SIZE))
      X_full.append(im)
      y_full.append(img_number)
      img_paths_full.append(img_path)


  
print(len(X_full))
print(X_full[0].shape)

print(len(y_full))
print(y_full[0])


# Double check the images
plt.figure()
plt.imshow(X_full[0])
plt.title(y_full[0])
plt.figure()
plt.imshow(X_full[1])
plt.title(y_full[1])

"""Convert the datasets to tensors"""

# conver to tensors
X_full = tf.convert_to_tensor(X_full)
y_full = tf.convert_to_tensor(y_full)

assert len(X_full) == len(y_full)
print('{} length data'.format(len(X_full)))

# Shuffle into train & val

indices = tf.range(start=0, limit=len(X_full), dtype=tf.int32)
shuffled_indices = tf.random.shuffle(indices)
X_full = tf.gather(X_full, shuffled_indices)
y_full = tf.gather(y_full, shuffled_indices)


# IF WE WANT A SEPARATE TEST SET
split_index = int(0.8 * len(X_full))
split_index_2 = int(0.1*len(X_full)) + split_index;
X_train, y_train, X_val, y_val, X_test, y_test = X_full[:split_index], y_full[:split_index], X_full[split_index:split_index_2], y_full[split_index:split_index_2], X_full[split_index_2:], y_full[split_index_2:]


# # IF WE WANT ONLY TRAIN AND VAL
# split_index = int(0.9 * len(X_full))
# X_train, y_train, X_val, y_val = X_full[:split_index], y_full[:split_index], X_full[split_index:], y_full[split_index:]

print(X_train.shape)
print(y_train.shape)
print(X_val.shape)
print(y_val.shape)
print(X_test.shape)
print(y_test.shape)

# See which classes are present in val & test

from collections import Counter

print("val")
y_val_array = np.asarray(y_val)
unique_rows, counts = (np.unique(y_val_array, axis=0, return_counts=True))
print(unique_rows)
print(counts)
print("test")
y_test_array = np.asarray(y_test)
unique_rows, counts = (np.unique(y_test_array, axis=0, return_counts=True))
print(unique_rows)
print(counts)

# Write the image files used in each set to files
print(img_paths_full[0])
img_paths_full = tf.gather(img_paths_full, shuffled_indices)
print(bytes.decode(img_paths_full[0].numpy()))

# Split into train val test
paths_train, paths_val, paths_test = img_paths_full[:split_index], img_paths_full[split_index:split_index_2], img_paths_full[split_index_2:]

# Get the date for later saving
from datetime import date
today = date.today()
date_string = today.strftime("%m%d%y")


with open(f'gdrive/My Drive/Danino_Lab/Patterning_Scans/cheW_umoD_Expts/models_and_histories/{date_string}_trainims.txt', 'w') as writefile:
    for line in paths_train:
        img_path = bytes.decode(line.numpy())
        writefile.write(img_path + "\n")

with open(f'gdrive/My Drive/Danino_Lab/Patterning_Scans/cheW_umoD_Expts/models_and_histories/{date_string}_valims.txt', 'w') as writefile:
    for line in paths_val:
        img_path = bytes.decode(line.numpy())
        writefile.write(img_path + "\n")

with open(f'gdrive/My Drive/Danino_Lab/Patterning_Scans/cheW_umoD_Expts/models_and_histories/{date_string}_testims.txt', 'w') as writefile:
    for line in paths_test:
        img_path = bytes.decode(line.numpy())
        writefile.write(img_path + "\n")

"""new approach is with a datagenerator"""

# Switching to a datagenerator
# DON't rescale will do later
train_datagen = ImageDataGenerator(
    rotation_range= 0.5,
    horizontal_flip= True,
    width_shift_range=0.1,
    height_shift_range=0.1)
    # width_shift_range=0.2,
    # height_shift_range=0.2)
# featurewise_center=True,
# featurewise_std_normalization=True,

# let's test the image data generator
temp_gen = train_datagen.flow(X_train, y_train, batch_size=32)
temp = temp_gen.next()
print(len(temp))

data_augmentations = tf.keras.Sequential([
    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
    tf.keras.layers.experimental.preprocessing.RandomRotation(0.5, fill_mode='constant'),
    tf.keras.layers.experimental.preprocessing.RandomTranslation(0.1, 0.1, fill_mode='constant')
])

"""# Define the model



"""

RESHAPED_SIZE = 224
base_model = EfficientNetB0(weights="imagenet", include_top=False, input_shape=(RESHAPED_SIZE, RESHAPED_SIZE, 3))
print("Number of layers in the base model: ", len(base_model.layers))

def build_efficientnet(architecture='EfficientNetB0', pretrained=False, finetune=True, data_augmentation=True, load_from=None):
    
    base_model = None
    preprocessing = None
    RESHAPED_SIZE = 224
    # Note that all the efficientnets do the rescaling from 0-255 to 0-1 internally
    # Download architecture w/ pretrained weights and freeze them
    if architecture.lower() == 'EfficientNetB0'.lower():
      base_model = EfficientNetB0(weights="imagenet" if pretrained else None, include_top=False, input_shape=(RESHAPED_SIZE, RESHAPED_SIZE, 3))
    elif architecture.lower() == 'EfficientNetB1'.lower():
      base_model = EfficientNetB1(weights="imagenet" if pretrained else None, include_top=False, input_shape=(RESHAPED_SIZE, RESHAPED_SIZE, 3))
    elif architecture.lower() == 'EfficientNetB2'.lower():
      base_model = EfficientNetB2(weights="imagenet" if pretrained else None, include_top=False, input_shape=(RESHAPED_SIZE, RESHAPED_SIZE, 3))
    elif architecture.lower() == 'EfficientNetB3'.lower():
      base_model = EfficientNetB3(weights="imagenet" if pretrained else None, include_top=False, input_shape=(RESHAPED_SIZE, RESHAPED_SIZE, 3))
    elif architecture.lower() == 'EfficientNetB4'.lower():
      base_model = EfficientNetB4(weights="imagenet" if pretrained else None, include_top=False, input_shape=(RESHAPED_SIZE, RESHAPED_SIZE, 3))
    elif architecture.lower() == 'EfficientNetB5'.lower():
      base_model = EfficientNetB5(weights="imagenet" if pretrained else None, include_top=False, input_shape=(RESHAPED_SIZE, RESHAPED_SIZE, 3))
    elif architecture.lower() == 'EfficientNetB6'.lower():
      base_model = EfficientNetB6(weights="imagenet" if pretrained else None, include_top=False, input_shape=(RESHAPED_SIZE, RESHAPED_SIZE, 3))
    elif architecture.lower() == 'EfficientNetB7'.lower():
      base_model = EfficientNetB7(weights="imagenet" if pretrained else None, include_top=False, input_shape=(RESHAPED_SIZE, RESHAPED_SIZE, 3))

    # Let's build the rest of the model
    preprocessing = tf.keras.Sequential([tf.keras.layers.Resizing(RESHAPED_SIZE, RESHAPED_SIZE)])

    # Specify how much we want to fine-tune
    if pretrained and finetune: 
      # Let's take a look to see how many layers are in the base model
      print("Number of layers in the base model: ", len(base_model.layers))
      # Fine-tune from this layer onwards
      fine_tune_at = len(base_model.layers) // 3
      # Freeze all the layers before the `fine_tune_at` layer
      for layer in base_model.layers[:fine_tune_at]:
          layer.trainable = False
    elif pretrained:
      base_model.trainable = False

    # Build the classification head for regression--2 outputs, linear activation only
    classification_head = tf.keras.Sequential([
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(2)
        # No activation specified should give us a linear activation output
    ])

    # Start to put together the full model, specifying augmentations if desired
    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    if data_augmentation:
        x = data_augmentations(inputs)
    else:
        x = inputs

    x = preprocessing(x)
    x = base_model(x, training=True)
    outputs = classification_head(x)
    model = tf.keras.Model(inputs, outputs)

    # compile the model
    learning_rate = 0.0001
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        learning_rate,
        decay_steps=100000,
        decay_rate=0.96)
    model.compile(
        # optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
        # THIS IS THE OTHER CHANGE--going to use MSE for regression output
        loss = tf.keras.losses.MeanSquaredError(),
        metrics = ['MeanAbsoluteError'],
        steps_per_execution = 1
    )

    return model

#Since our data generator has augmentation, we won't add it into the models here

try_models = {
    # 'b0': build_efficientnet(architecture = 'EfficientNetB0', pretrained=True, finetune=True, data_augmentation=False),
    # 'b1': build_efficientnet(architecture = 'EfficientNetB1', pretrained=True, finetune=True, data_augmentation=False),
    'b2': build_efficientnet(architecture = 'EfficientNetB2', pretrained=True, finetune=True, data_augmentation=False)
    # 'b3': build_efficientnet(architecture = 'EfficientNetB3', pretrained=True, finetune=True, data_augmentation=False)
    # 'b4': build_efficientnet(architecture = 'EfficientNetB4', pretrained=True, finetune=True, data_augmentation=False)
    # 'b5': build_efficientnet(architecture = 'EfficientNetB5', pretrained=True, finetune=True, data_augmentation=False),
    # 'b6': build_efficientnet(architecture = 'EfficientNetB6', pretrained=True, finetune=True, data_augmentation=False),
    # 'b7': build_efficientnet(architecture = 'EfficientNetB7', pretrained=True, finetune=True, data_augmentation=False),
}

# Define early stopping
early_stopping = callbacks.EarlyStopping(monitor="val_loss", patience=4)

# Get the date for later saving
from datetime import date
today = date.today()
date_string = today.strftime("%m%d%y")

# Let's fit all these models
# Evaluate on the val dataset the MSE for each
from sklearn.metrics import mean_squared_error

# y_pred = model.predict(X_test)
# print("MSE: %.4f" % mean_squared_error(y_test, y_pred))


batch_size = 32
epochs = 50
histories = {}
val_results = {}

for model_key, model in try_models.items():

    # Define saving best model only checkpoint
    checkpoint_filepath = os.path.join(f'gdrive/My Drive/Danino_Lab/Patterning_Scans/cheW_umoD_Expts/models_and_histories/{date_string}_{model_key}_alldata', 'ckpt') #, f'weights.{epoch:02d}-{val_loss:.2f}.hdf5')
    # checkpoint_filepath = 'gdrive/My Drive/Danino_Lab/Patterning_Scans/cheW_umoD_Expts/{date_string}_{model_key}'
    model_checkpoint_callback = callbacks.ModelCheckpoint(
      filepath=checkpoint_filepath,
      save_weights_only=True,
      monitor='val_loss',
      mode='min',
      save_best_only=True)


    print(f'\n\n Training {model_key} \n\n')


    histories[model_key] = model.fit(
      train_datagen.flow(X_train, y_train, batch_size=batch_size),
      epochs=epochs,
      callbacks=[model_checkpoint_callback],
      steps_per_epoch=int(len(X_train) / batch_size),
      validation_data = (X_val, y_val))
    

    # save the history
    with open(f'gdrive/My Drive/Danino_Lab/Patterning_Scans/cheW_umoD_Expts/models_and_histories/{date_string}_{model_key}_all_data_history_file', 'wb') as file_pi:
      pickle.dump(histories[model_key].history, file_pi)

# Store the MSE on the validation set as well
y_pred = model.predict(X_val)
val_results[model_key] = mean_squared_error(y_val, y_pred)
print(val_results)

y_pred = model.predict(X_test)
test_results = {}
test_results[model_key] = mean_squared_error(y_test, y_pred)
print(test_results)

"""# Review the results

Generate a plot of the training mean squared error and mean absolute errors throughout, add legend
Plot the MSE on val dataset for each model

"""

import seaborn as sns
sns.set_style('white')
sns.set_theme()

def plot_multiple(history, model_name, linecolor, ALPHA, MARKER):
  
  # The history object contains results on the training and test
  # sets for each epoch
  acc = history['mean_absolute_error']
  val_acc = history['val_mean_absolute_error']
  loss = history['loss']
  val_loss = history['val_loss']

  min_val_loss = min(val_loss)
  best_epoch = val_loss.index(min_val_loss) + 1

  max_loss = max(loss)

  # Get the number of epochs
  epochs_completed = len(loss)
  epochs_range = range(1,epochs_completed+1)
  
  # loss
  plt.subplot(1, 2, 1)
  plt.plot(epochs_range, loss, color=linecolor, linestyle='dashed', alpha = ALPHA, label=f'Train {model_name}')
  plt.plot(epochs_range, val_loss, color=linecolor, alpha = ALPHA, label=f'Val {model_name}')
  plt.scatter(x=best_epoch,y=min_val_loss,marker=MARKER,alpha = ALPHA,c=linecolor)
  plt.ylim([0, max_loss+0.05])

  # acc
  plt.subplot(1, 2, 2)
  plt.plot(epochs_range, acc, color=linecolor, linestyle='dashed', alpha = ALPHA, label=f'Train {model_name}')
  plt.plot(epochs_range, val_acc, color=linecolor, alpha = ALPHA, label=f'Val {model_name}')

def format_histories_plot(max_epochs):
  plt.subplot(1, 2, 1)
  plt.title('Mean Squared Error')
  plt.xlabel('Epoch',fontsize=10)
  plt.ylabel('MSQ',fontsize=10)
  plt.xticks(ticks=np.arange(1,max_epochs+1,4))
  plt.yticks(ticks=np.arange(0,0.45,0.1))
  #plt.legend(bbox_to_anchor=(1.33, 1))
  plt.legend(facecolor='white',edgecolor='gray')

  plt.subplot(1, 2, 2)
  plt.title('Mean Absolute Error')
  plt.xlabel('Epoch',fontsize=10)
  plt.ylabel('MAE',fontsize=10)
  plt.xticks(ticks=np.arange(1,max_epochs+1,4))
  plt.yticks(ticks=np.arange(0,0.6,0.2))
  #plt.legend(bbox_to_anchor=(1.33, 1))



  fig.suptitle('Regression on cheWumoD Images with Efficientnets trained on All Data',y=0.91,fontweight='bold')

fig, ax = plt.subplots(figsize = [30, 20])
idx = 0
palette = sns.color_palette("viridis", 25).as_hex()
#palette_2 = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000']
palette_2 = ['#e6194b', '#3cb44b', '#911eb4', '#46f0f0', '#e6194b', '#3cb44b', '#911eb4', '#46f0f0']
all_epochs_completed = list()

for model_key, history_obj in histories.items():
  model_name = model_key
  history = history_obj.history
  eps = len(history['loss'])
  all_epochs_completed.append(eps)
  ALPHA = 1
  MARKER = 'o'
  LINESTYLE = 'solid'
  linecolor = palette_2[idx]
  plot_multiple(history, model_name, linecolor, ALPHA, MARKER)
  idx += 1

max_epochs = np.amax(all_epochs_completed)
format_histories_plot(max_epochs)
plt.show()