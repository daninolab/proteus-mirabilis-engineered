{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Full Implementation of VGG-11 U-Net Segmentation Model\nTraning, validation, and testing of a pre-trained VGG-11 U-Net on *Proteus mirabilis* pLac-*flgM* colony images for segmentating ring boundaries where changes in environmental conditions occurred.\n\n**References/Acknowledgments:** This implementation follows the ring boundary segmentation model we previously presented in Doshi & Shaw et al. 2022<sup>1,2</sup>. Whereas the task of the initial work was to segment all ring boundaries within a *P. mirabilis* colony image, the model is re-trained and evaluated here for the new task of segmenting only the boundaries that delineate changes in environmental conditions.  \n\nThe architecture and pretrained VGG-11 encoder of the U-Net, as well as several utility functions, are imported from \"Segmentation Models: Python library with Neural Networks for Image Segmentation based on PyTorch\" (SMP)<sup>3</sup>. For both ring boundary segmentation tasks, our scripts were adapted from the SMP car segmentation example, benefiting from its specified functions for data loading, augmentation, and model training.  \n\n[1] Doshi, A.\\*\\, M. Shaw\\*\\, R. Tonea, R. Minyety, S. Moon, A. Laine, J. Guo\\^\\, and T. Danino\\^\\. A deep learning pipeline for segmentation of *Proteus mirabilis* colony patterns. in *2022 IEEE 19th\nInternational Symposium on Biomedical Imaging (ISBI)*. 2022. IEEE. doi: 10.1109/ISBI52829.2022.9761643\n\n[2] daninolab. mirabilis-ringboundary-seg-minimal. 2022; Available from: https://github.com/daninolab/proteus-mirabilis.\n\n[3] Iakubovskii, P. segmentation_models.pytorch (Version 0.2.0). 2021; Available from: https://github.com/qubvel/segmentation_models.pytorch.","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# Earlier PyPI version (0.2.0) that we have been using: \n!pip install segmentation-models-pytorch==0.2.0\n# To get the latest version from source:\n#!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n    \n!pip install openpyxl # for reading in excel file","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:53:45.197195Z","iopub.execute_input":"2023-02-01T23:53:45.197750Z","iopub.status.idle":"2023-02-01T23:53:55.652944Z","shell.execute_reply.started":"2023-02-01T23:53:45.197653Z","shell.execute_reply":"2023-02-01T23:53:55.652317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport csv\nimport copy\nimport time\nfrom tqdm import tqdm\nimport os\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision import models\nfrom torch.utils.data import Dataset, DataLoader\nimport glob\nimport matplotlib.pyplot as plt\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch import losses\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\nimport albumentations as albu\nimport math\nfrom skimage.morphology import skeletonize, thin\nfrom skimage import data\nfrom skimage.util import invert","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:54:37.548569Z","iopub.execute_input":"2023-02-01T23:54:37.549001Z","iopub.status.idle":"2023-02-01T23:54:37.556133Z","shell.execute_reply.started":"2023-02-01T23:54:37.548975Z","shell.execute_reply":"2023-02-01T23:54:37.555593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# Specify train, val, test sizes\nnum_train = 13\nnum_val = 4\nnum_test = 4","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:54:15.472338Z","iopub.execute_input":"2023-02-01T23:54:15.472681Z","iopub.status.idle":"2023-02-01T23:54:15.476454Z","shell.execute_reply.started":"2023-02-01T23:54:15.472654Z","shell.execute_reply":"2023-02-01T23:54:15.475559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the paths to the input image and mask folders\nimg_dir = '../input/flgm-updated-110721/x_preProc'\nmask_dir = '../input/flgm-updated-110721/y'","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:54:16.263333Z","iopub.execute_input":"2023-02-01T23:54:16.263663Z","iopub.status.idle":"2023-02-01T23:54:16.267299Z","shell.execute_reply.started":"2023-02-01T23:54:16.263637Z","shell.execute_reply":"2023-02-01T23:54:16.266305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load in train-val-test split list (excel file)\ntrain_val_test_path = '../input/flgm-temp-changes-trainvaltestlist-082021/flgM_TrainValTest_list.xlsx'\ntrain_val_test_df = pd.read_excel(train_val_test_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:57:28.137793Z","iopub.execute_input":"2023-02-01T23:57:28.138077Z","iopub.status.idle":"2023-02-01T23:57:28.162679Z","shell.execute_reply.started":"2023-02-01T23:57:28.138051Z","shell.execute_reply":"2023-02-01T23:57:28.161920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to extract img IDs for train, val, & test sets\ndef get_train_val_test_IDs(train_val_test_df, num_train, num_val, num_test):\n    \n    train_col = train_val_test_df.loc[0:num_train-1,[\"Train\"]].values\n    val_col = train_val_test_df.loc[0:num_val-1:,[\"Val\"]].values\n    test_col = train_val_test_df.loc[0:num_test-1:,[\"Test\"]].values\n    \n    train_IDs = [str(train_img)[2:-2] for train_img in train_col]\n    val_IDs = [str(val_img)[2:-2] for val_img in val_col]\n    test_IDs = [str(test_img)[2:-2] for test_img in test_col]\n    \n    return train_IDs, val_IDs, test_IDs","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:57:28.817284Z","iopub.execute_input":"2023-02-01T23:57:28.817585Z","iopub.status.idle":"2023-02-01T23:57:28.823432Z","shell.execute_reply.started":"2023-02-01T23:57:28.817559Z","shell.execute_reply":"2023-02-01T23:57:28.822504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the img IDs\ntrain_IDs, val_IDs, test_IDs = get_train_val_test_IDs(train_val_test_df, num_train, num_val, num_test)\n\n# Confirm dataset sizes\nprint(len(train_IDs))\nprint(len(val_IDs))\nprint(len(test_IDs))","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:57:29.584241Z","iopub.execute_input":"2023-02-01T23:57:29.584561Z","iopub.status.idle":"2023-02-01T23:57:29.598767Z","shell.execute_reply.started":"2023-02-01T23:57:29.584533Z","shell.execute_reply":"2023-02-01T23:57:29.597347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset class\nclass BacteriaDataset(Dataset):\n    \n    CLASSES = ['boundaries']\n    \n    def __init__(self, img_IDs, img_dir, mask_dir, \n                 classes=None,augmentation=None, preprocessing=None):\n        self.img_IDs = img_IDs\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir  \n        self.augmentation = augmentation         # for augmentations\n        self.preprocessing = preprocessing       # preprocessing to normalize images\n        \n         # convert str names to class values on masks\n        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n        \n    def __len__(self):\n        return len(self.img_IDs)\n\n    def __getitem__(self, i):\n        \n        # read data\n        img_path = os.path.join(self.img_dir, self.img_IDs[i])\n        mask_path = os.path.join(self.mask_dir, self.img_IDs[i].replace(\".tif\",\"_testim_boundarymask_uncrop.tif\"))\n        \n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n        mask = (mask >= 1).astype('float32')\n        mask = np.expand_dims(mask, axis=2) \n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=img, mask=mask)\n            img, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=img, mask=mask)\n            img, mask = sample['image'], sample['mask']\n            \n        return img, mask","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:57:31.669203Z","iopub.execute_input":"2023-02-01T23:57:31.669636Z","iopub.status.idle":"2023-02-01T23:57:31.678145Z","shell.execute_reply.started":"2023-02-01T23:57:31.669607Z","shell.execute_reply":"2023-02-01T23:57:31.677633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformations definitions \n# For training set:\ndef get_training_augmentation():\n    train_transform = [albu.PadIfNeeded(min_height=1024, min_width=1024, always_apply=True, border_mode=cv2.BORDER_REFLECT_101),\n                       albu.Rotate(limit=(-10,10), border_mode=cv2.BORDER_REFLECT_101, p=0.5),\n                       albu.HorizontalFlip(p=0.5),\n                       albu.VerticalFlip(p=0.5),\n                       albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0, rotate_limit=0,\n                                          border_mode=cv2.BORDER_REFLECT_101, p=0.5), # translate\n                       albu.ShiftScaleRotate(shift_limit=0, scale_limit=0.5, rotate_limit=0,\n                                          border_mode=cv2.BORDER_REFLECT_101, p=0.5), # zoom\n                      ]\n    return albu.Compose(train_transform)\n\n# For validation and test sets:\ndef get_val_test_augmentation():\n    val_test_transform = [\n                       albu.PadIfNeeded(min_height=1024, min_width=1024, always_apply=True, border_mode=cv2.BORDER_REFLECT_101),\n                      ]\n    return albu.Compose(val_test_transform)\n\n# Necessary for feeding images into model\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing(preprocessing_fn):\n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:57:34.116556Z","iopub.execute_input":"2023-02-01T23:57:34.116919Z","iopub.status.idle":"2023-02-01T23:57:34.123335Z","shell.execute_reply.started":"2023-02-01T23:57:34.116894Z","shell.execute_reply":"2023-02-01T23:57:34.122827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(15, 10))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image,cmap='binary',vmin=0,vmax=1)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:57:36.936126Z","iopub.execute_input":"2023-02-01T23:57:36.936450Z","iopub.status.idle":"2023-02-01T23:57:36.942496Z","shell.execute_reply.started":"2023-02-01T23:57:36.936420Z","shell.execute_reply":"2023-02-01T23:57:36.941376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at a randon image+mask pair from our train dataset\norig_train_set = BacteriaDataset(train_IDs, img_dir, mask_dir,classes=['boundaries'])\nrand_num = np.random.choice(len(orig_train_set))\nimg, mask = orig_train_set[rand_num] \nfilename = train_IDs[rand_num]\nprint(filename)\nvisualize(original_pattern_image=img/255, \n          ground_truth_mask=mask.squeeze()\n         )","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:57:38.499021Z","iopub.execute_input":"2023-02-01T23:57:38.499413Z","iopub.status.idle":"2023-02-01T23:57:38.910933Z","shell.execute_reply.started":"2023-02-01T23:57:38.499379Z","shell.execute_reply":"2023-02-01T23:57:38.910272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize 5 transformed images+masks from our train dataset\n# (no training augmentationd used in this run)\ntrain_dataset = BacteriaDataset(train_IDs, img_dir, mask_dir, classes=['boundaries'],\n                                augmentation=get_val_test_augmentation(),\n                                # comment out this preprocessing line, as it's only needed for loading images into model:\n                                #preprocessing=get_preprocessing(preprocess_input), \n                                )\n\nfor i in range(5):\n    n = np.random.choice(len(train_dataset))\n    img, mask = train_dataset[n]\n    filename = train_IDs[n]\n    print(filename)\n    visualize(transformed_pattern_image=img/255, \n              transformed_ground_truth_mask=mask)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:57:42.036214Z","iopub.execute_input":"2023-02-01T23:57:42.036652Z","iopub.status.idle":"2023-02-01T23:57:43.807212Z","shell.execute_reply.started":"2023-02-01T23:57:42.036624Z","shell.execute_reply":"2023-02-01T23:57:43.806497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model architecture and hyperparameters","metadata":{}},{"cell_type":"code","source":"# Set some variables \n\n# For saving results\ndate_started = '110721'\narchitecture_name = 'vgg11_UNet_flgm' \nmodel_name = architecture_name + '_' + date_started\nprint(model_name)\n\n# If loading in model from previous round of training, to continute training here\nload_model = False\n\n# Specific to model implementation\nEncoder = 'vgg11'\nAttention = None \nWeights = 'imagenet'\nACTIVATION = 'sigmoid'\nCLASSES = ['boundaries']\npreprocess_input = get_preprocessing_fn(Encoder, Weights)\nmore_epochs = 60 # How many epochs/more epochs to train for\npatience = 3 # For early stopping\ntrain_batch_size = 3\nval_batch_size = 1\ntest_batch_size = 1","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:57:53.519006Z","iopub.execute_input":"2023-02-01T23:57:53.519276Z","iopub.status.idle":"2023-02-01T23:57:53.524772Z","shell.execute_reply.started":"2023-02-01T23:57:53.519252Z","shell.execute_reply":"2023-02-01T23:57:53.523982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create segmentation model with pretrained encoder\n# https://github.com/qubvel/segmentation_models.pytorch\nmodel = smp.Unet(\n    encoder_name=Encoder, \n    encoder_weights=Weights, \n    decoder_attention_type=Attention,\n    in_channels=3, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:57:56.702792Z","iopub.execute_input":"2023-02-01T23:57:56.703062Z","iopub.status.idle":"2023-02-01T23:58:01.160720Z","shell.execute_reply.started":"2023-02-01T23:57:56.703038Z","shell.execute_reply":"2023-02-01T23:58:01.159996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize loss, metrics, % optimizer:\nloss = smp.utils.losses.DiceLoss()\n\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n    smp.utils.metrics.Fscore(),\n    smp.utils.metrics.Accuracy(),\n    smp.utils.metrics.Recall(),\n    smp.utils.metrics.Precision()\n]\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001),\n])","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:58:02.963881Z","iopub.execute_input":"2023-02-01T23:58:02.964307Z","iopub.status.idle":"2023-02-01T23:58:02.971228Z","shell.execute_reply.started":"2023-02-01T23:58:02.964281Z","shell.execute_reply":"2023-02-01T23:58:02.970680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training & Validation","metadata":{}},{"cell_type":"code","source":"# File where previous history would have been stored, if resuming model training\ntrainvalCSVname = model_name + '_TrainValcsv.csv' \n\n# Upload previous model & scores if resuming training \nif load_model is True:\n    model_name_dir = model_name.replace(\"_\", \"-\").lower()\n    model_dir = '../input/' + model_name_dir + '-models-logs'\n    dfTrainVal_path = os.path.join(model_dir, trainvalCSVname)\n    dfTrainVal = pd.read_csv(dfTrainVal_path)\n    \n    resume_epoch = len(dfTrainVal.index)\n    last_epoch = resume_epoch - 1\n    \n    last_checkpoint_name = model_name + '_epoch_' + str(last_epoch) + '.pth'\n    last_checkpoint_path = os.path.join(model_dir, last_checkpoint_name)\n    last_checkpoint = torch.load(last_checkpoint_path)\n    \n    model.load_state_dict(last_checkpoint['model_state_dict'])\n    optimizer.load_state_dict(last_checkpoint['optimizer_state_dict'])\n# Otherwise, create new dataframe for storing metrics    \nelse: \n    dfTrainVal = pd.DataFrame(columns=['Epoch', \n                                   'Train Loss','Val Loss', \n                                   'Train Accuracy','Val Accuracy', \n                                   'Train Precision','Val Precision', \n                                   'Train Recall','Val Recall', \n                                   'Train IoU','Val IoU', \n                                   'Train Fscore','Val Fscore'])\n    resume_epoch = len(dfTrainVal.index)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:58:05.437899Z","iopub.execute_input":"2023-02-01T23:58:05.438201Z","iopub.status.idle":"2023-02-01T23:58:05.447177Z","shell.execute_reply.started":"2023-02-01T23:58:05.438165Z","shell.execute_reply":"2023-02-01T23:58:05.446344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view the dataframe, whether it's empty or filled with previous history \ndfTrainVal","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:58:07.242159Z","iopub.execute_input":"2023-02-01T23:58:07.242433Z","iopub.status.idle":"2023-02-01T23:58:07.256322Z","shell.execute_reply.started":"2023-02-01T23:58:07.242410Z","shell.execute_reply":"2023-02-01T23:58:07.255549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create transformed & preprocessed datasets\ntrain_dataset = BacteriaDataset(train_IDs, img_dir, mask_dir, classes=['boundaries'],\n                                # since we're not using train augmentations here,... \n                                # we can just use the standard transformations needed for all images here \n                                augmentation=get_val_test_augmentation(),\n                                preprocessing=get_preprocessing(preprocess_input),\n                               )\n\nval_dataset = BacteriaDataset(val_IDs, img_dir, mask_dir,classes=['boundaries'],\n                              augmentation=get_val_test_augmentation(),\n                              preprocessing=get_preprocessing(preprocess_input),\n                             )\n\ntest_dataset = BacteriaDataset(test_IDs, img_dir, mask_dir,classes=['boundaries'],\n                              augmentation=get_val_test_augmentation(),\n                              preprocessing=get_preprocessing(preprocess_input),\n                              )\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=12)\nval_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:58:08.024853Z","iopub.execute_input":"2023-02-01T23:58:08.025116Z","iopub.status.idle":"2023-02-01T23:58:08.032056Z","shell.execute_reply.started":"2023-02-01T23:58:08.025092Z","shell.execute_reply":"2023-02-01T23:58:08.031258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore DataLoader\nprint('Training data Info:')\ndataiter = iter(train_loader)\ndata = dataiter.next()\nimages,labels = data\nprint(\"shape of images : {}\".format(images.shape))\nprint(\"shape of labels : {}\".format(labels.shape))\n\nprint('\\nValidation data Info:')\ndataiter = iter(val_loader)\ndata = dataiter.next()\nimages,labels = data\nprint(\"shape of images : {}\".format(images.shape))\nprint(\"shape of labels : {}\".format(labels.shape))\n\nprint('\\nTest data Info:')\ndataiter = iter(test_loader)\ndata = dataiter.next()\nimages,labels = data\nprint(\"shape of images : {}\".format(images.shape))\nprint(\"shape of labels : {}\".format(labels.shape))","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:58:09.182750Z","iopub.execute_input":"2023-02-01T23:58:09.183286Z","iopub.status.idle":"2023-02-01T23:58:09.848287Z","shell.execute_reply.started":"2023-02-01T23:58:09.183243Z","shell.execute_reply":"2023-02-01T23:58:09.847280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create epoch runners, as done in https://github.com/qubvel/segmentation_models.pytorch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntrain_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=device,\n    verbose=True,\n)\n\nval_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=device,\n    verbose=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:58:12.614698Z","iopub.execute_input":"2023-02-01T23:58:12.614986Z","iopub.status.idle":"2023-02-01T23:58:12.628359Z","shell.execute_reply.started":"2023-02-01T23:58:12.614959Z","shell.execute_reply":"2023-02-01T23:58:12.627519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start (or continue) train & validating model\n\nEPOCHS = resume_epoch + more_epochs\nes = 0 # initiliaze early stopping counter\n\nfor epoch in range(resume_epoch, EPOCHS):\n    \n    print('\\nEpoch: {}'.format(epoch))\n    train_logs = train_epoch.run(train_loader)\n    val_logs = val_epoch.run(val_loader)\n    \n    # Determine what the previous min val loss was \n    if epoch == 0:\n        min_val_loss = 1\n    else:\n        min_val_loss = dfTrainVal['Val Loss'].min()\n\n    # Update the dataframe with scores from this epoch\n    dfTrainVal.loc[epoch, ['Epoch']] = epoch\n    dfTrainVal.loc[epoch, ['Train Loss']] = train_logs['dice_loss']\n    dfTrainVal.loc[epoch, ['Val Loss']] = val_logs['dice_loss']\n    dfTrainVal.loc[epoch, ['Train Accuracy']] = train_logs['accuracy']\n    dfTrainVal.loc[epoch, ['Val Accuracy']] = val_logs['accuracy']\n    dfTrainVal.loc[epoch, ['Train Precision']] = train_logs['precision']\n    dfTrainVal.loc[epoch, ['Val Precision']] = val_logs['precision']\n    dfTrainVal.loc[epoch, ['Train Recall']] = train_logs['recall']\n    dfTrainVal.loc[epoch, ['Val Recall']] = val_logs['recall']\n    dfTrainVal.loc[epoch, ['Train IoU']] = train_logs['iou_score']\n    dfTrainVal.loc[epoch, ['Val IoU']] = val_logs['iou_score']\n    dfTrainVal.loc[epoch, ['Train Fscore']] = train_logs['fscore']\n    dfTrainVal.loc[epoch, ['Val Fscore']] = val_logs['fscore']\n    \n    # Save the dataframe\n    dfTrainVal.to_csv(trainvalCSVname,index=False)\n    \n    # Save model checkpoints\n    checkpoint = {'epoch': epoch,\n                  'model_state_dict': model.state_dict(),\n                  'optimizer_state_dict': optimizer.state_dict(),\n                  'loss': loss}\n    checkpoint_path = './'+model_name+'_epoch_'+str(epoch)+'.pth'\n    torch.save(checkpoint, checkpoint_path)\n    \n    # Early stopping: check if val loss has decreased/increased from the previous min_val_loss\n    val_loss = val_logs['dice_loss']\n    if val_loss < min_val_loss:\n        es = 0 # Early stopping not considered\n    else: \n        es += 1 # Start counting\n        print(\"EarlyStopping Counter {} of {}\".format(es,patience))\n        \n        if es >= patience:\n            print(\"Early stopping with min_val_loss: \", min_val_loss, \"and val_loss for this epoch: \", val_loss, \"...\")\n            break","metadata":{"execution":{"iopub.status.busy":"2023-02-01T23:58:13.814159Z","iopub.execute_input":"2023-02-01T23:58:13.814437Z","iopub.status.idle":"2023-02-02T00:01:01.380181Z","shell.execute_reply.started":"2023-02-01T23:58:13.814411Z","shell.execute_reply":"2023-02-02T00:01:01.379333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find early stopping point\nstop_pt =  dfTrainVal[['Val Loss']].astype(float).idxmin()[0]\nprint(stop_pt)\n\n# Determine how many epochs were completed\nepochs_completed = dfTrainVal.shape[0]\nprint(epochs_completed)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T00:01:10.558061Z","iopub.execute_input":"2023-02-02T00:01:10.558360Z","iopub.status.idle":"2023-02-02T00:01:10.565408Z","shell.execute_reply.started":"2023-02-02T00:01:10.558328Z","shell.execute_reply":"2023-02-02T00:01:10.564829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test the best saved model","metadata":{}},{"cell_type":"code","source":"# First, initialize the model & the optimizer \nbest_model = smp.Unet(\n    encoder_name=Encoder, \n    encoder_weights=Weights, \n    decoder_attention_type=Attention,\n    in_channels=3,\n    classes=len(CLASSES), \n    activation=ACTIVATION,\n)\n\nbest_optimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001),\n])","metadata":{"execution":{"iopub.status.busy":"2023-02-02T00:01:13.887988Z","iopub.execute_input":"2023-02-02T00:01:13.888524Z","iopub.status.idle":"2023-02-02T00:01:16.229013Z","shell.execute_reply.started":"2023-02-02T00:01:13.888469Z","shell.execute_reply":"2023-02-02T00:01:16.228251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the epoch with min val loss & load in checkpoints for that epoch\nbest_epoch = dfTrainVal[['Val Loss']].astype(float).idxmin()[0]\n\nif best_epoch < resume_epoch:\n    # previously uploaded checkpoint\n    best_checkpoint_name = model_name + '_epoch_' + str(best_epoch) + '.pth'\n    best_checkpoint_path = os.path.join(model_dir, best_checkpoint_name)\nelse:\n    # newly saved checkpoint\n    best_checkpoint_path = './' + model_name + '_epoch_' + str(best_epoch) + '.pth'\n\nbest_checkpoint = torch.load(best_checkpoint_path)\nbest_model.load_state_dict(best_checkpoint['model_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2023-02-02T00:01:17.879788Z","iopub.execute_input":"2023-02-02T00:01:17.880118Z","iopub.status.idle":"2023-02-02T00:01:17.940460Z","shell.execute_reply.started":"2023-02-02T00:01:17.880084Z","shell.execute_reply":"2023-02-02T00:01:17.939774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate model on test set\ntest_epoch = smp.utils.train.ValidEpoch(\n    model=best_model,\n    loss=loss,\n    metrics=metrics,\n    device=device,\n)\n\ntest_logs = test_epoch.run(test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T00:01:18.811860Z","iopub.execute_input":"2023-02-02T00:01:18.812154Z","iopub.status.idle":"2023-02-02T00:01:34.209231Z","shell.execute_reply.started":"2023-02-02T00:01:18.812126Z","shell.execute_reply":"2023-02-02T00:01:34.208306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up & save dataframe for storing test scores \ndfTest = pd.DataFrame(columns=[\n                           'Test Loss',\n                           'Test Accuracy',\n                           'Test Precision',\n                           'Test Recall',\n                           'Test IoU',\n                           'Test Fscore'])\n\ndfTest.loc[0, ['Test Loss']] = test_logs['dice_loss']\ndfTest.loc[0, ['Test Accuracy']] = test_logs['accuracy']\ndfTest.loc[0, ['Test Precision']] = test_logs['precision']\ndfTest.loc[0, ['Test Recall']] = test_logs['recall']\ndfTest.loc[0, ['Test IoU']] = test_logs['iou_score']\ndfTest.loc[0, ['Test Fscore']] = test_logs['fscore']\n\ntestCSVname = model_name + '_epoch_'+str(best_epoch)+ '_Testcsv.csv'\ndfTest.to_csv(testCSVname,index=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T00:02:25.235812Z","iopub.execute_input":"2023-02-02T00:02:25.236083Z","iopub.status.idle":"2023-02-02T00:02:25.249850Z","shell.execute_reply.started":"2023-02-02T00:02:25.236058Z","shell.execute_reply":"2023-02-02T00:02:25.249251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show test results\ndfTest","metadata":{"execution":{"iopub.status.busy":"2023-02-02T00:02:26.091923Z","iopub.execute_input":"2023-02-02T00:02:26.092394Z","iopub.status.idle":"2023-02-02T00:02:26.102815Z","shell.execute_reply.started":"2023-02-02T00:02:26.092363Z","shell.execute_reply":"2023-02-02T00:02:26.102068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize predictions","metadata":{}},{"cell_type":"code","source":"# Function for generating predicted mask (cropped back down to the size of originak image: 1000x1000)\n# & skeletonized version of cropped predicted mask\n# ...given an index, a dataset, & a model\n\ndef generate_prediction_skel(n, dataset, model):\n    # Get transformed (padded) + preprocessed image\n    image = dataset[n][0]  \n    img_tensor = torch.from_numpy(image).to(device).unsqueeze(0)\n    \n    # Generate prediction\n    pr_mask = model.predict(img_tensor)\n    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n    cropped_pr_mask = pr_mask[12:1012, 12:1012]\n    \n    # Skeletonize the mask\n    skeleton = skeletonize(cropped_pr_mask)\n    skeleton = skeleton.astype(np.float32)\n    \n    return cropped_pr_mask, skeleton","metadata":{"execution":{"iopub.status.busy":"2023-02-02T00:02:29.316785Z","iopub.execute_input":"2023-02-02T00:02:29.317112Z","iopub.status.idle":"2023-02-02T00:02:29.322108Z","shell.execute_reply.started":"2023-02-02T00:02:29.317085Z","shell.execute_reply":"2023-02-02T00:02:29.321571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test dataset without transformations for image visualization\ntest_dataset_vis = BacteriaDataset(test_IDs, img_dir, mask_dir, classes=['boundaries'],)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T00:02:30.389858Z","iopub.execute_input":"2023-02-02T00:02:30.390258Z","iopub.status.idle":"2023-02-02T00:02:30.394136Z","shell.execute_reply.started":"2023-02-02T00:02:30.390225Z","shell.execute_reply":"2023-02-02T00:02:30.393473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create output folder for storing cropped predicted masks\npred_folder = 'predictions'\nif not os.path.exists(pred_folder):\n    os.makedirs(pred_folder)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T00:02:31.312406Z","iopub.execute_input":"2023-02-02T00:02:31.313269Z","iopub.status.idle":"2023-02-02T00:02:31.317147Z","shell.execute_reply.started":"2023-02-02T00:02:31.313230Z","shell.execute_reply":"2023-02-02T00:02:31.316631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create output folder for storing skeletonized cropped predicted masks\nskel_folder = 'skel_predictions'\nif not os.path.exists(skel_folder):\n    os.makedirs(skel_folder)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T00:02:32.582181Z","iopub.execute_input":"2023-02-02T00:02:32.582658Z","iopub.status.idle":"2023-02-02T00:02:32.586845Z","shell.execute_reply.started":"2023-02-02T00:02:32.582621Z","shell.execute_reply":"2023-02-02T00:02:32.586091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize all predictions on test set\ntest_size = len(test_IDs)\n\nfor i in range(test_size):\n    \n    # Get the image filename \n    filename = test_IDs[i]\n    print(filename) # so I know which images I'm viewing\n    filename_wo_ext = os.path.splitext(os.path.basename(filename))[0]\n    \n    # Visualize untransformed+unpreprocessed input image + ground truth mask\n    image_vis, gt_vis = test_dataset_vis[i]\n    gt_vis = gt_vis.squeeze()\n    visualize(original_pattern_image=image_vis/255,\n              ground_truth_mask=gt_vis,)\n    \n    # Generate and save cropped predicted mask & skeletonized version \n    cropped_pr_mask, skeleton = generate_prediction_skel(i, test_dataset, best_model)\n    pred_filename = filename_wo_ext + '_pred_ep' + str(best_epoch) + '.tif'\n    pred_path = os.path.join(pred_folder, pred_filename)\n    cv2.imwrite(pred_path, cropped_pr_mask)\n    skel_filename = filename_wo_ext + '_skel_ep' + str(best_epoch) + '.tif'\n    skel_path = os.path.join(skel_folder, skel_filename)\n    cv2.imwrite(skel_path, skeleton)\n    \n    # Visualize cropped predicted mask & skeletonized version \n    visualize(predicted_mask=cropped_pr_mask,\n              skeletonized_predicted_mask=skeleton,)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T00:02:42.496919Z","iopub.execute_input":"2023-02-02T00:02:42.497386Z","iopub.status.idle":"2023-02-02T00:03:01.907563Z","shell.execute_reply.started":"2023-02-02T00:02:42.497353Z","shell.execute_reply":"2023-02-02T00:03:01.906998Z"},"trusted":true},"execution_count":null,"outputs":[]}]}